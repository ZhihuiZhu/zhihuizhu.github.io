<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Zhihui Zhu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="Publications.html">Publications</a></div>
<div class="menu-item"><a href="Teaching.html">Teaching</a></div>
<div class="menu-item"><a href="People.html">People</a></div>
<div class="menu-item"><a href="Talks.html">Talks</a></div>
<div class="menu-item"><a href="Software.html">Software</a></div>
<div class="menu-item"><a href="zhuCV.pdf">CV</a></div>
</td>
<td id="layout-content">
<p><tt> </tt> </p>
<table class="imgtable"><tr><td>
<img src="pic/ZZ.jpg" alt="alt text" width="384px" height="296px" />&nbsp;</td>
<td align="left"><p>Zhihui Zhu<br />
Assistant Professor<br />
<a href="https://https://cse.osu.edu//">Computer Science and Engineering</a><br />
<a href="https://www.osu.edu/">The Ohio State University</a><br /></p>
<p><br />
583 Dreese Lab<br />
2015 Neil Avenue<br />
Columbus, OH 43210<br /></p>
<p><br />
Phone: <br />
Email: zhu.3440@osu.edu</p>
</td></tr></table>
<p><br /></p>
<p><b>I am looking for self-motivated students who are interested in  data science, machine learning, and signal processing.</b>	If you are interested in joining my lab, please send me your CV, transcripts, and any other demonstration materials.<br /> </p>
<p>Our group works on theoretical and computational aspects of data science and machine learning, with a focus on developing efficient methods for extracting useful information from large-scale and high-dimensional data, proving their correctness, and applying them to solving real-world problems. A few current projects include </p>
<ul>
<li><p><b>Machine learning</b>: representation learning; generalization properties and implicit regularization; model (neural network) compression; deep neural networks for unsupervised learning and inverse problems;</p>
</li>
<li><p><b>Quantum information</b>: statistical and algorithmic aspects of quantum information;</p>
</li>
<li><p><b>Optimization</b>: Nonconvex geometric analysis; distributed optimization; the design, analysis, and implementation of large-scale optimization algorithms for engineering problems.<br /></p>
</li>
</ul>
<h2>News: </h2>
<ul>
<li><p>[Jan 2023] Co-organized and gave a <a href="https://buckeyemailosu-my.sharepoint.com/:b:/g/personal/zhu_3440_osu_edu/ES8L3-DSGThGjIV6FenNXIoBwDI-_SCWdFONoebTie17zA?e=099OWe">tutorail</a> at the 3rd SLowDNN Workshop at MBZUAI, Abu Dhabi, MBZUAI. </p>
</li>
</ul>
<ul>
<li><p>[Dec 2022] Received <a href="https://quantum.osu.edu/">CQISE</a> Partnership Seed Award (PSA) and will collaborate with Brian Kirby (ARL) on quantum network. </p>
</li>
</ul>
<ul>
<li><p>[Dec 2022] Invited to serve as area chair at ICML 2023.</p>
</li>
</ul>
<ul>
<li><p>[Nov 2022] Elected to serve on the Technical Committee of the Machine Learning for Signal Processing (MLSP TC) under the IEEE Signal Processing Society. </p>
</li>
</ul>
<ul>
<li><p>[Sep 2022] Four papers accetped to <a href="https://nips.cc/">NeurIPS 2022</a>. </p>
</li>
</ul>
<ul>
<li><p>[August 2022] Our group joined the Department of Computer Science and Engineering at The Ohio State University.</p>
</li>
</ul>
<ul>
<li><p>[May 2022] On May 26-27, together with <a href="https://sites.google.com/view/jsulam/about">Jere's group</a>, we had our annual Deep & Sparse Team meeting at the University of Denver to discuss project accomplishments and plans.</p>
</li>
</ul>
<ul>
<li><p>[May 2022] Gave a 10-hours short course (with Sam Buchanan, Yi Ma, Qing Qu, John Wright, and Yuqian Zhang) on &lsquo;&lsquo;<a href="https://highdimdata-lowdimmodels-tutorial.github.io/">Low-Dimensional Models for High-Dimensional Data: From Linear to Nonlinear, Convex to Nonconvex, and Shallow to Deep</a>" at  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).</p>
</li>
</ul>
<ul>
<li><p>[May 2021] Invited to serve as a TPC member (area chair) at NeurIPS 2022.</p>
</li>
</ul>
<ul>
<li><p>[Dec 2021] Invited to serve as Action Editor at the <a href="https://jmlr.org/tmlr/">Transactions on Machine Learning Research</a>.</p>
</li>
</ul>
<ul>
<li><p>[Nov 2021] Co-organizing the 2nd workshop on &lsquo;&lsquo;<a href="https://sites.google.com/view/slowdnn2021/">Seeking Low-dimensionality in Deep Neural Networks (SLowDNN)</a>&rsquo;&rsquo;, Nov. 22nd – Nov. 23rd, 2021.</p>
</li>
</ul>
<ul>
<li><p>[Oct 2021] Four papers accetped to <a href="https://nips.cc/">NeurIPS 2021</a>. </p>
</li>
</ul>
<ul>
<li><p>[May 2021] One ICML&rsquo;21 on robust subspace learned accepted. New <a href="https://arxiv.org/abs/2105.02375">paper</a> released on understanding the behabior of the classifiers in monder deep neural networks.</p>
</li>
</ul>
<ul>
<li><p>[May 2021] Our <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2106881&amp;HistoricalAwards=false">collaborative proposal</a> (with Mike and Gongguo at CSM) on Structured Inference and Adaptive Measurement Design has been awarded by NSF!</p>
</li>
</ul>
<ul>
<li><p>[March 2021] Invited to serve as a TPC member (area chair) at NeurIPS 2021.</p>
</li>
</ul>
<ul>
<li><p>[March 2021] Invited <a href="https://www.dropbox.com/s/lsvxde2msteh9mx/Robust_recovery.pdf?dl=0">talk</a> at Microsoft. One paper on <a href="https://arxiv.org/abs/2103.00673">Convolutional Normalization</a> is on arxiv. One CVPR&rsquo;21 on <a href="https://arxiv.org/abs/2103.10559">frame interpolation</a> and one AISTATS&rsquo;21 on hyperplane clustering accepted.</p>
</li>
</ul>
<ul>
<li><p>[Nov 2020] Co-organizing IEEE workshop on &lsquo;&lsquo;<a href="https://sites.google.com/view/slowdnn/">Seeking Low-dimensionality in Deep Neural Networks (SLowDNN)</a>&rsquo;&rsquo;, Nov. 23rd – Nov. 24th, 2020.</p>
</li>
</ul>
<ul>
<li><p>[Sep 2020] Our <a href="https://arxiv.org/pdf/2006.08857.pdf">paper</a> has been accepted at NeurIPS as spotlight (top 4%), which characterizes implicit bias with discrepant learning rates and builds connections between over-parameterization, RPCA, and deep neural networks.</p>
</li>
</ul>
<ul>
<li><p>[Jun 2020] Our proposal (with <a href="https://sites.google.com/view/jsulam/about">Jere</a> at JHU) &lsquo;&lsquo;Collaborative Research: CIF: Small: Deep Sparse Models: Analysis and Algorithms&rsquo;&rsquo; has been awarded by NSF!</p>
</li>
</ul>
<ul>
<li><p>[Jun 2020] Two papers about over-parameterization are on arXiv: <a href="https://arxiv.org/pdf/2006.06179.pdf">one</a> studies the benefit of over-realized model in dictionary learning, <a href="https://arxiv.org/pdf/2006.08857.pdf">another one</a> characterizes implicit bias with discrepant learning rates and builds connection between over-parameterization, RPCA, and deep neural networks.</p>
</li>
</ul>
<ul>
<li><p>[Feb 2020] Our paper on robust homography estimation has been accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>.</p>
</li>
</ul>
<ul>
<li><p>[Jan 2020] Co-organized with Qing and Shuyang, our two-session mini-symposium &lsquo;&lsquo;Recent Advances in Optimization Methods for Signal Processing and Machine Learning&rsquo;&rsquo; has been accepted by the inaugural <i><a href="https://www.siam.org/conferences/cm/conference/mds20">SIAM Conference on Mathematics of Data Science</a></i>. See you at Cincinnati, Ohio in May!</p>
</li>
</ul>
<ul>
<li><p>[Jan 2020] Our review paper (with Qing, Xiao, Manolis, John, and Rene) &lsquo;&lsquo;<a href="https://arxiv.org/abs/2001.06970">Finding the Sparsest Vectors in a Subspace: Theory, Algorithms, and Applications</a>&rsquo;&rsquo; is on arxiv.</p>
</li>
</ul>
<ul>
<li><p>[Jan 2020] Invited talk at <a href="https://sites.google.com/colorado.edu/statoptml/">Statistics, Optimization and Machine Learning Seminar</a>, University of Colorado Boulder.</p>
</li>
</ul>
<ul>
<li><p>[Jan 2020] Invited talk at Colorado School of Mines.</p>
</li>
</ul>
<ul>
<li><p>[Dec 2019 ] Our paper <a href="https://arxiv.org/abs/1809.09237">Nonconvex Robust Low-rank Matrix Recovery</a> has been accepted by <i>SIAM Journal on Optimization</i>.</p>
</li>
</ul>
<ul>
<li><p>[Dec 2019 ] Our paper <a href="https://openreview.net/forum?id=rygixkHKDH">Analysis of the Optimization Landscapes for Overcomplete Representation Learning</a> was accepted to <a href="https://iclr.cc/">ICLR 2020</a> and was selected for <font color=red ><b>oral</b></font> presentation. In this paper, we showed benign optimization landscapes for learning overcomplete/convolutional dictionaries, ensuring simple gradient descent find the targeted solutions.</p>
</li>
</ul>
<ul>
<li><p>[Dec 2019 ] Attended <a href="https://nips.cc/">NeurIPS 2019</a> and presented 3 papers. </p>
</li>
</ul>
<ul>
<li><p>[Dec 2019] Invited talk at <a href="http://www.inspirelab.us/seminars/">Signal and Information Processing Seminar</a>, Rutgers University.</p>
</li>
</ul>
<ul>
<li><p>[Nov 2019] Our paper (with Xiao, Shixiang, Zengde, Qing, and Anthony) &lsquo;&lsquo;<a href="https://arxiv.org/abs/1911.05047">Nonsmooth Optimization over Stiefel Manifold: Riemannian Subgradient Methods</a>&rsquo;&rsquo; is on arxiv. This work provides (first) explicit convergence rate guarantees for a family of Riemannian subgradient methods when used to optimize nonsmooth functions (that are weakly convex in the Euclidean space) over then Stiefel manifold.</p>
</li>
</ul>
<ul>
<li><p>[Oct 2019] Attended the Northrop Grumman University Research Symposium, and presented our work on &lsquo;&lsquo;Object Identification with Less Supervision".</p>
</li>
</ul>
<ul>
<li><p>[Oct 2019] Attended the <a href="https://ima.umn.edu/2019-2020/SW10.14-18.19">Computational Imaging</a> workshop at IMA, University of Minnesota, and presented our work on &lsquo;&lsquo;A Linearly Convergent Method for Non-smooth Non-convex Optimization on Grassmannian with Applications to Robust Subspace and Dictionary Learning&rsquo;&rsquo;.</p>
</li>
</ul>
<ul>
<li><p>[Sep 2019] Gave an invited talk on <a href="talks/LARM-Sep19-Zhu.pdf">Provable Nonconvex Approaches for Low-rank Models</a> at <a href="https://sites.google.com/site/lowrankmodels/">Workshop on Low-Rank Models and Applications (LRMA)</a>, University of Mons, Belgium, Sep 12 – 13, 2019. </p>
</li>
</ul>
<ul>
<li><p>[Sep 2019] 3 papers accepted to <a href="https://nips.cc/">NeurIPS 2019</a>.</p>
</li>
</ul>
<ul>
<li><p>[Aug 2019] Gave an invited talk at <a href="https://iccopt2019.berlin/">ICCOPT 2019, the Sixth International Conference on Continuous Optimization</a>, Technical University (TU) of Berlin, Aug 3 – 8, 2019.</p>
</li>
</ul>
<ul>
<li><p>[Aug 2019] Our paper (with Xiao, Anthony, Jason) &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1907.11687.pdf">Incremental Methods for Weakly Convex Optimization</a>&rsquo;&rsquo; is on arxiv. This work provides (first) convergence guarantee for incrememtal algorithms and their random shuffling version (including the incremental subgradient method which is the work-horse of deep learning) in solving weakly convex optimization problems which could be nonconvex and nonsmooth.</p>
</li>
</ul>
<ul>
<li><p>[Aug 2019] Our paper (with Qing, Xiao) &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1908.10776.pdf">A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution</a>&rsquo;&rsquo; is on arxiv. This work considers multichannel sparse blind deconvolution problem and provides efficient first-order methods that can exactly solve this blind deconvolution problem in a linear rate.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2023-01-17 22:44:54 EST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
