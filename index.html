<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Zhihui Zhu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="Publications.html">Publications</a></div>
<div class="menu-item"><a href="Teaching.html">Teaching</a></div>
<div class="menu-item"><a href="People.html">People</a></div>
<div class="menu-item"><a href="Talks.html">Talks</a></div>
<div class="menu-item"><a href="Software.html">Software</a></div>
<div class="menu-item"><a href="Openings.html">Openings</a></div>
<div class="menu-item"><a href="zhuCV.pdf">CV</a></div>
</td>
<td id="layout-content">
<p>


<tt> </tt> 



</p>
<table class="imgtable"><tr><td>
<img src="pic/ZZ.jpg" alt="alt text" width="384px" height="296px" />&nbsp;</td>
<td align="left"><p>Zhihui Zhu<br />
Assistant Professor<br />
<a href="https://ritchieschool.du.edu/" target=&ldquo;blank&rdquo;>Electrical and Computer Engineering</a><br />
<a href="https://www.du.edu/" target=&ldquo;blank&rdquo;>University of Denver</a><br />

<br />
Engineering and Computer Science Building<br />
126C<br />
2155 E. Wesley Avenue<br />
Denver CO 80208, USA<br />

<br />
Phone: (303) 871-5249<br />
Email: zhihui.zhu@du.edu
</p>
</td></tr></table>
<p>(Photo by SL, at Maroon Bells Aspen, September 2016)<br />

<br />

<b>I am looking for self-motivated students who are interested in  data science, machine learning, optimization, and signal processing.</b>	If you are interested in joining my lab, please send me your CV, transcripts, and any demonstration materials such as papers or drafts.<br /> 

<br />

I am an Assistant Professor with the Department of Electrical and Computer Engineering, University of Denver. I was a Postdoctoral Fellow in the <a href="https://www.minds.jhu.edu/" target=&ldquo;blank&rdquo;>Mathematical Institute for Data Science</a> at The <a href="https://www.jhu.edu/" target=&ldquo;blank&rdquo;>Johns Hopkins University</a>. I obtained my Ph.D. degreee in  <a href="http://inside.mines.edu/EE-home" target=&ldquo;blank&rdquo;>Electrical Engineering</a> from the <a href="https://www.mines.edu/" target=&ldquo;blank&rdquo;>Colorado School of Mines</a> in 2017. My research focuses on the interaction among the fields of signal processing, data analysis, and machine learning, using tools from optimization, approximation theory, and harmonic analysis. I am especially interested in efficient and reliable methods for extracting useful information in large-scale and high-dimensional signals and data. A few current projects include deep neural networks for unsupervised learning and inverse problems; landscape analysis of  (nonsmooth) nonconvex optimizations; efficient and provably correct algorithms in the offline, stochastic, or distributed settings by exploiting the geometric properties; theoretically understanding of deep learning.




</p>
<h2>News:
</h2>
<p>
</p>
<ul>
<li><p>[Dec 2021] Invited to serve as Action Editor at the <a href="https://jmlr.org/tmlr/" target=&ldquo;blank&rdquo;>Transactions on Machine Learning Research</a>.

</p>
</li>
<li><p>[Nov 2021] Co-organizing the 2nd workshop on &lsquo;&lsquo;<a href="https://sites.google.com/view/slowdnn2021/" target=&ldquo;blank&rdquo;>Seeking Low-dimensionality in Deep Neural Networks (SLowDNN)</a>&rsquo;&rsquo;, Nov. 22nd – Nov. 23rd, 2021.

</p>
</li>
<li><p>[Oct 2021] Four papers accetped to <a href="https://nips.cc/" target=&ldquo;blank&rdquo;>NeurIPS 2021</a>. 

</p>
</li>
<li><p>[May 2021] One ICML&rsquo;21 on robust subspace learned accepted. New <a href="https://arxiv.org/abs/2105.02375" target=&ldquo;blank&rdquo;>paper</a> released on understanding the behabior of the classifiers in monder deep neural networks.

</p>
</li>
<li><p>[May 2021] Our <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2106881&amp;HistoricalAwards=false" target=&ldquo;blank&rdquo;>collaborative proposal</a> (with Mike and Gongguo at CSM) on Structured Inference and Adaptive Measurement Design has been awarded by NSF!

</p>
</li>
<li><p>[March 2021] Invited to serve as a TPC member (area chair) at NeurIPS 2021.

</p>
</li>
<li><p>[March 2021] Invited <a href="https://www.dropbox.com/s/lsvxde2msteh9mx/Robust_recovery.pdf?dl=0" target=&ldquo;blank&rdquo;>talk</a> at Microsoft. One paper on <a href="https://arxiv.org/abs/2103.00673" target=&ldquo;blank&rdquo;>Convolutional Normalization</a> is on arxiv. One CVPR&rsquo;21 on <a href="https://arxiv.org/abs/2103.10559" target=&ldquo;blank&rdquo;>frame interpolation</a> and one AISTATS&rsquo;21 on hyperplane clustering accepted.

</p>
</li>
<li><p>[Nov 2020] Co-organizing IEEE workshop on &lsquo;&lsquo;<a href="https://sites.google.com/view/slowdnn/" target=&ldquo;blank&rdquo;>Seeking Low-dimensionality in Deep Neural Networks (SLowDNN)</a>&rsquo;&rsquo;, Nov. 23rd – Nov. 24th, 2020.

</p>
</li>
<li><p>[Sep 2020] Our <a href="https://arxiv.org/pdf/2006.08857.pdf" target=&ldquo;blank&rdquo;>paper</a> has been accepted at NeurIPS as spotlight (top 4%), which characterizes implicit bias with discrepant learning rates and builds connections between over-parameterization, RPCA, and deep neural networks.

</p>
</li>
<li><p>[Jun 2020] Our proposal (with <a href="https://sites.google.com/view/jsulam/about" target=&ldquo;blank&rdquo;>Jere</a> at JHU) &lsquo;&lsquo;Collaborative Research: CIF: Small: Deep Sparse Models: Analysis and Algorithms&rsquo;&rsquo; has been awarded by NSF!

</p>
</li>
<li><p>[Jun 2020] Two papers about over-parameterization are on arXiv: <a href="https://arxiv.org/pdf/2006.06179.pdf" target=&ldquo;blank&rdquo;>one</a> studies the benefit of over-realized model in dictionary learning, <a href="https://arxiv.org/pdf/2006.08857.pdf" target=&ldquo;blank&rdquo;>another one</a> characterizes implicit bias with discrepant learning rates and builds connection between over-parameterization, RPCA, and deep neural networks.

</p>
</li>
<li><p>[Feb 2020] Our paper on robust homography estimation has been accepted to <a href="http://cvpr2020.thecvf.com/" target=&ldquo;blank&rdquo;>CVPR 2020</a>.

</p>
</li>
<li><p>[Jan 2020] Co-organized with Qing and Shuyang, our two-session mini-symposium &lsquo;&lsquo;Recent Advances in Optimization Methods for Signal Processing and Machine Learning&rsquo;&rsquo; has been accepted by the inaugural <i><a href="https://www.siam.org/conferences/cm/conference/mds20" target=&ldquo;blank&rdquo;>SIAM Conference on Mathematics of Data Science</a></i>. See you at Cincinnati, Ohio in May!

</p>
</li>
<li><p>[Jan 2020] Our review paper (with Qing, Xiao, Manolis, John, and Rene) &lsquo;&lsquo;<a href="https://arxiv.org/abs/2001.06970" target=&ldquo;blank&rdquo;>Finding the Sparsest Vectors in a Subspace: Theory, Algorithms, and Applications</a>&rsquo;&rsquo; is on arxiv.

</p>
</li>
<li><p>[Jan 2020] Invited talk at <a href="https://sites.google.com/colorado.edu/statoptml/" target=&ldquo;blank&rdquo;>Statistics, Optimization and Machine Learning Seminar</a>, University of Colorado Boulder.

</p>
</li>
<li><p>[Jan 2020] Invited talk at Colorado School of Mines.

</p>
</li>
<li><p>[Dec 2019 ] Our paper <a href="https://arxiv.org/abs/1809.09237" target=&ldquo;blank&rdquo;>Nonconvex Robust Low-rank Matrix Recovery</a> has been accepted by <i>SIAM Journal on Optimization</i>.

</p>
</li>
<li><p>[Dec 2019 ] Our paper <a href="https://openreview.net/forum?id=rygixkHKDH" target=&ldquo;blank&rdquo;>Analysis of the Optimization Landscapes for Overcomplete Representation Learning</a> was accepted to <a href="https://iclr.cc/" target=&ldquo;blank&rdquo;>ICLR 2020</a> and was selected for <font color=red ><b>oral</b></font> presentation. In this paper, we showed benign optimization landscapes for learning overcomplete/convolutional dictionaries, ensuring simple gradient descent find the targeted solutions.

</p>
</li>
<li><p>[Dec 2019 ] Attended <a href="https://nips.cc/" target=&ldquo;blank&rdquo;>NeurIPS 2019</a> and presented 3 papers. 

</p>
</li>
<li><p>[Dec 2019] Invited talk at <a href="http://www.inspirelab.us/seminars/" target=&ldquo;blank&rdquo;>Signal and Information Processing Seminar</a>, Rutgers University.

</p>
</li>
<li><p>[Nov 2019] Our paper (with Xiao, Shixiang, Zengde, Qing, and Anthony) &lsquo;&lsquo;<a href="https://arxiv.org/abs/1911.05047" target=&ldquo;blank&rdquo;>Nonsmooth Optimization over Stiefel Manifold: Riemannian Subgradient Methods</a>&rsquo;&rsquo; is on arxiv. This work provides (first) explicit convergence rate guarantees for a family of Riemannian subgradient methods when used to optimize nonsmooth functions (that are weakly convex in the Euclidean space) over then Stiefel manifold.

</p>
</li>
<li><p>[Oct 2019] Attended the Northrop Grumman University Research Symposium, and presented our work on &lsquo;&lsquo;Object Identification with Less Supervision".

</p>
</li>
<li><p>[Oct 2019] Attended the <a href="https://ima.umn.edu/2019-2020/SW10.14-18.19" target=&ldquo;blank&rdquo;>Computational Imaging</a> workshop at IMA, University of Minnesota, and presented our work on &lsquo;&lsquo;A Linearly Convergent Method for Non-smooth Non-convex Optimization on Grassmannian with Applications to Robust Subspace and Dictionary Learning&rsquo;&rsquo;.

</p>
</li>
<li><p>[Sep 2019] Gave an invited talk on <a href="talks/LARM-Sep19-Zhu.pdf" target=&ldquo;blank&rdquo;>Provable Nonconvex Approaches for Low-rank Models</a> at <a href="https://sites.google.com/site/lowrankmodels/" target=&ldquo;blank&rdquo;>Workshop on Low-Rank Models and Applications (LRMA)</a>, University of Mons, Belgium, Sep 12 – 13, 2019. 

</p>
</li>
<li><p>[Sep 2019] 3 papers accepted to <a href="https://nips.cc/" target=&ldquo;blank&rdquo;>NeurIPS 2019</a>.

</p>
</li>
<li><p>[Aug 2019] Gave an invited talk at <a href="https://iccopt2019.berlin/" target=&ldquo;blank&rdquo;>ICCOPT 2019, the Sixth International Conference on Continuous Optimization</a>, Technical University (TU) of Berlin, Aug 3 – 8, 2019.

</p>
</li>
<li><p>[Aug 2019] Our paper (with Xiao, Anthony, Jason) &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1907.11687.pdf" target=&ldquo;blank&rdquo;>Incremental Methods for Weakly Convex Optimization</a>&rsquo;&rsquo; is on arxiv. This work provides (first) convergence guarantee for incrememtal algorithms and their random shuffling version (including the incremental subgradient method which is the work-horse of deep learning) in solving weakly convex optimization problems which could be nonconvex and nonsmooth.

</p>
</li>
<li><p>[Aug 2019] Our paper (with Qing, Xiao) &lsquo;&lsquo;<a href="https://arxiv.org/pdf/1908.10776.pdf" target=&ldquo;blank&rdquo;>A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution</a>&rsquo;&rsquo; is on arxiv. This work considers multichannel sparse blind deconvolution problem and provides efficient first-order methods that can exactly solve this blind deconvolution problem in a linear rate.

















</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-01-15 09:00:00 MST, by <a href="https://github.com/wsshin/jemdoc_mathjax" target="blank">jemdoc+MathJax</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>

